{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "# import joblib\n",
    "import pickle\n",
    "# Define preprocessing function\n",
    "import re\n",
    "import nltk\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Define preprocessing function\n",
    "from nltk.corpus import stopwords\n",
    "#import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n",
    "from re import sub\n",
    "import html as ihtml\n",
    "from itertools import groupby\n",
    "# from emot.emo_unicode import EMOTICONS_EMO\n",
    "import emoji\n",
    "# lemmatize words with spacy\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url_pattern.sub(r'', text)\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = html_pattern.sub(r'', text)\n",
    "    text = BeautifulSoup(ihtml.unescape(text)).text\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Dictionary of English Contractions\n",
    "    # Dictionary of English Contractions\n",
    "    contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"can not\",\"can't've\": \"can not have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\", \n",
    "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\",\"dont\":\"do not\",\"wont\":\"will not\",\"cant\":\"can not\", \"aint\":\"am not\", \"isnt\":\"is not\", \"doesnt\":\"does not\", \"didnt\":\"did not\", \"shouldnt\":\"should not\", \"wouldnt\":\"would not\", \"couldnt\":\"could not\", \"havent\":\"have not\", \"hasnt\":\"has not\", \"hadnt\":\"had not\", \"im\":\"i am\", \n",
    "                     \"ive\":\"i have\", \"youve\":\"you have\", \"youre\":\"you are\", \"theyve\":\"they have\", \"theyre\":\"they are\", \"weve\":\"we have\", \"were\":\"we are\", \"whats\":\"what is\", \"wheres\":\"where is\",\n",
    "                     \"whens\":\"when is\", \"whys\":\"why is\", \"hows\":\"how is\", \"theres\":\"there is\", \"theres\":\"there is\", \"whos\":\"who is\"\n",
    "                                }\n",
    "\n",
    "    # Regular expression for finding contractions\n",
    "    contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "\n",
    "    def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "        def replace(match):\n",
    "            return contractions_dict[match.group(0)]\n",
    "        return contractions_re.sub(replace, s)\n",
    "        \n",
    "    text = expand_contractions(text)\n",
    "    \n",
    "    text = text.upper()\n",
    "    chat_words_str = \"\"\"\n",
    "    AFAIK=As Far As I Know\n",
    "    AFK=Away From Keyboard\n",
    "    ASAP=As Soon As Possible\n",
    "    ATK=At The Keyboard\n",
    "    ATM=At The Moment\n",
    "    A3=Anytime, Anywhere, Anyplace\n",
    "    BAK=Back At Keyboard\n",
    "    BBL=Be Back Later\n",
    "    BBS=Be Back Soon\n",
    "    BFN=Bye For Now\n",
    "    B4N=Bye For Now\n",
    "    BRB=Be Right Back\n",
    "    BRT=Be Right There\n",
    "    BTW=By The Way\n",
    "    B4=Before\n",
    "    B4N=Bye For Now\n",
    "    CU=See You\n",
    "    CUL8R=See You Later\n",
    "    CYA=See You\n",
    "    FAQ=Frequently Asked Questions\n",
    "    FC=Fingers Crossed\n",
    "    FWIW=For What It's Worth\n",
    "    FYI=For Your Information\n",
    "    GAL=Get A Life\n",
    "    GG=Good Game\n",
    "    GN=Good Night\n",
    "    GMTA=Great Minds Think Alike\n",
    "    GR8=Great!\n",
    "    G9=Genius\n",
    "    IC=I See\n",
    "    ICQ=I Seek you (also a chat program)\n",
    "    ILU=ILU: I Love You\n",
    "    IMHO=In My Honest/Humble Opinion\n",
    "    IMO=In My Opinion\n",
    "    IOW=In Other Words\n",
    "    IRL=In Real Life\n",
    "    KISS=Keep It Simple, Stupid\n",
    "    LDR=Long Distance Relationship\n",
    "    LMAO=Laugh My A.. Off\n",
    "    LOL=Laughing Out Loud\n",
    "    LTNS=Long Time No See\n",
    "    L8R=Later\n",
    "    MTE=My Thoughts Exactly\n",
    "    M8=Mate\n",
    "    NRN=No Reply Necessary\n",
    "    OIC=Oh I See\n",
    "    PITA=Pain In The A..\n",
    "    PRT=Party\n",
    "    PRW=Parents Are Watching\n",
    "    ROFL=Rolling On The Floor Laughing\n",
    "    ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "    ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "    SK8=Skate\n",
    "    STATS=Your sex and age\n",
    "    ASL=Age, Sex, Location\n",
    "    THX=Thank You\n",
    "    TTFN=Ta-Ta For Now!\n",
    "    TTYL=Talk To You Later\n",
    "    U=You\n",
    "    U2=You Too\n",
    "    U4E=Yours For Ever\n",
    "    WB=Welcome Back\n",
    "    WTF=What The F...\n",
    "    WTG=Way To Go!\n",
    "    WUF=Where Are You From?\n",
    "    W8=Wait...\n",
    "    7K=Sick:-D Laugher\n",
    "    &=and\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    text = ''.join(''.join(s)[:2] for _, s in groupby(text))\n",
    "\n",
    "    # chat_words = {}\n",
    "    # for line in chat_words_str.splitlines():\n",
    "    #     if line:\n",
    "    #         key, value = line.split('=')\n",
    "    #         chat_words[key] = value\n",
    "\n",
    "    # # Replace chat words with their full form\n",
    "    # for key, value in chat_words.items():\n",
    "    #     text = text.replace(key, value)\n",
    "\n",
    "    text = text.replace(\"@[A-Za-z0-9]+\",\"\")\n",
    "\n",
    "    # for emot in EMOTICONS_EMO:\n",
    "    #     text = text.replace(emot, \" \".join(EMOTICONS_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    \n",
    "    def extract_emojis(text):\n",
    "        new_text = []\n",
    "        new_text.append(emoji.demojize(text, delimiters=(\"\", \"\")))\n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    text = extract_emojis(text)\n",
    "\n",
    "    text = text.replace(\"_\", \" \")\n",
    "\n",
    "    text = text.replace(\"-\", \" \")\n",
    "\n",
    "    text = text.replace(\"  \", \" \")\n",
    "\n",
    "    text = text.replace(r'\\b\\w\\b', '').replace(r'\\s+', ' ')\n",
    "\n",
    "    # def lemmatize_words(text):\n",
    "    #     return \" \".join([token.lemma_ for token in nlp(text)])\n",
    "    \n",
    "    # text = lemmatize_words(text)\n",
    "\n",
    "    text = text.replace('\\d+', '')\n",
    "\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    def remove_punctuation(text):\n",
    "        \"\"\"custom function to remove the punctuation\"\"\"\n",
    "        return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    \n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    all_stopwords.remove('nor')\n",
    "    all_stopwords.remove('no')\n",
    "    all_stopwords.remove('but')\n",
    "    all_stopwords.remove('too')\n",
    "    all_stopwords.remove('very')\n",
    "    all_stopwords.remove('just')\n",
    "    all_stopwords.remove('don')\n",
    "    all_stopwords.remove('doesn')\n",
    "    all_stopwords.remove('didn')\n",
    "    all_stopwords.remove('wasn')\n",
    "    all_stopwords.remove('weren')\n",
    "    all_stopwords.remove('isn')\n",
    "    all_stopwords.remove('aren')\n",
    "    all_stopwords.remove('haven')\n",
    "    all_stopwords.remove('hasn')\n",
    "    all_stopwords.remove('hadn')\n",
    "    all_stopwords.remove('won')\n",
    "    all_stopwords.remove('wouldn')\n",
    "    all_stopwords.remove('shouldn')\n",
    "    all_stopwords.remove('couldn')\n",
    "    all_stopwords.remove('mustn')\n",
    "    all_stopwords.remove('mightn')\n",
    "    all_stopwords.remove('needn')\n",
    "    all_stopwords.remove('shan')\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        return \" \".join([word for word in str(text).split() if word not in all_stopwords])\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    text = text.replace('[^\\w\\s]', '')\n",
    "\n",
    "    # text = text.apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from pickle file\n",
    "with open('tfidf.pkl', 'rb') as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "with open('LogisticAspect2.pkl','rb') as file2:\n",
    "    logistic_aspect = pickle.load(file2)\n",
    "with open('LogisticPolarity.pkl','rb') as file3:\n",
    "    logistic_polarity = pickle.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(sentence, aspect_model, polarity_model, tfidf_vectorizer):\n",
    "\n",
    "    # Split the input sentence into individual sentences using the full stop character\n",
    "    comment = preprocess(sentence)\n",
    "    # sentences = sentence.split('.')\n",
    "    \n",
    "\n",
    "    # Vectorize the preprocessed sentences\n",
    "    X = tfidf_vectorizer.transform([comment])\n",
    "\n",
    "    # Predict the aspects for each sentence using the aspect model\n",
    "    aspect_preds = aspect_model.predict(X)\n",
    "\n",
    "    # Classify the polarity of each sentence using the polarity model\n",
    "    polarity_preds = polarity_model.predict(X)\n",
    "    aspect =\"\"\n",
    "    aspect_pred = aspect_preds[0]\n",
    "    if np.any(aspect_pred[0]==1): aspect+= \" | Data integration\"\n",
    "    if np.any(aspect_pred[1]==1): aspect+= \" | Marketing, Communication & Special offers\"\n",
    "    if np.any(aspect_pred[2]==1): aspect+= \" | Technology\"\n",
    "    if np.any(aspect_pred[3]==1): aspect+= \" | Payment and Check-out\"\n",
    "    if np.any(aspect_pred[4]==1): aspect+= \" | Shopping experience\"\n",
    "    if np.any(aspect_pred[5]==1): aspect+= \" | Unemployment\"\n",
    "    if np.any(aspect_pred[6]==1): aspect+= \" | Product availability and Store design\"\n",
    "    if np.any(aspect_pred[7]==1): aspect+= \" | Price and Value\"\n",
    "    if np.any(aspect_pred[8]==1): aspect+= \" | General\"\n",
    "    if np.any(aspect_pred[9]==1): aspect+= \" | Privacy and Security issues\"\n",
    "    if(aspect!=\"\"): aspect = aspect[3:]\n",
    "\n",
    "    polarity = str(polarity_preds)\n",
    "    polarity = preprocess(polarity)\n",
    "    if(polarity==\"positi\"):polarity=\"Positive\"\n",
    "    elif(polarity==\"negati\"):polarity=\"Negative\"\n",
    "    elif(polarity==\"neutral\"): polarity=\"Neutral\"\n",
    "    return aspect, polarity\n",
    "\n",
    "def predict_aspect(text):\n",
    "    aspect_preds = logistic_aspect.predict(text)\n",
    "    aspect =\"\"\n",
    "    aspect_pred = aspect_preds[0]\n",
    "    if np.any(aspect_pred[0]==1): aspect+= \" | Data integration\"\n",
    "    if np.any(aspect_pred[1]==1): aspect+= \" | Marketing, Communication & Special offers\"\n",
    "    if np.any(aspect_pred[2]==1): aspect+= \" | Technology\"\n",
    "    if np.any(aspect_pred[3]==1): aspect+= \" | Payment and Check-out\"\n",
    "    if np.any(aspect_pred[4]==1): aspect+= \" | Shopping experience\"\n",
    "    if np.any(aspect_pred[5]==1): aspect+= \" | Unemployment\"\n",
    "    if np.any(aspect_pred[6]==1): aspect+= \" | Product availability and Store design\"\n",
    "    if np.any(aspect_pred[7]==1): aspect+= \" | Price and Value\"\n",
    "    if np.any(aspect_pred[8]==1): aspect+= \" | General\"\n",
    "    if np.any(aspect_pred[9]==1): aspect+= \" | Privacy and Security issues\"\n",
    "    if(aspect!=\"\"): aspect = aspect[3:]\n",
    "    return aspect\n",
    "\n",
    "\n",
    "def predict_polarity(vec):\n",
    "    polarity_preds = logistic_polarity.predict(vec)\n",
    "    polarity = str(polarity_preds)\n",
    "    polarity = preprocess(polarity)\n",
    "    if(polarity==\"positi\"):polarity=\"Positive\"\n",
    "    elif(polarity==\"negati\"):polarity=\"Negative\"\n",
    "    elif(polarity==\"neutral\"): polarity=\"Neutral\"\n",
    "    return polarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi_aspect(text):\n",
    "    # Tách câu dài hoặc đoạn nhiều câu bằng các từ như \"but\", \"except\", \"yet\", dấu chấm\n",
    "    sentences = re.split(r'[.!?;]|but |except |yet ', text)\n",
    "    \n",
    "    aspects = \"\"\n",
    "    polarities = \"\"\n",
    "    # Tiền xử lý và dự đoán các khía cạnh trong từng câu\n",
    "    for sentence in sentences:\n",
    "        # Tiền xử lý văn bản\n",
    "        preprocessed = preprocess(sentence)\n",
    "        vec = tfidf_vectorizer.transform([preprocessed])\n",
    "        aspect = predict_aspect(vec)\n",
    "        polarity = predict_polarity(vec)\n",
    "        # Dự đoán các khía cạnh trong câu\n",
    "        if aspect !=\"\":aspects+= \" | \"+aspect\n",
    "        polarities+=\" | \"+polarity\n",
    "    if(aspects!=\"\"): aspects = aspects[3:]\n",
    "    if(polarities!=\"\"): polarities = polarities[3:]\n",
    "        \n",
    "\n",
    "    \n",
    "    # Loại bỏ các câu null\n",
    "    \n",
    "    return aspects, polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('General', 'Negative | Positive')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_multi_aspect('I dont like the food. i like the store')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
